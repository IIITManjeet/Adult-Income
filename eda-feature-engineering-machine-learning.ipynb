{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":29840,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Adult Income : Exploratory Analysis And Precition   \n\nThis notebook has been created to help you go through the steps of a Machine Learning project Life-Cicle, from Business Understanding to presenting the final result to the Business.  \n\n## 1. Business Understanding \n## 2. Data aquisition  \n          Automatique Data aquisition  \n          Convert data into a Pandas Data Frame\n          \n## 3- Data Munging  \n          Treating missing values\n          Working with outliers\n          \n## 4- Exploratory Data Analysis \n          Univariate Analysis      \n          Bivariate analysis           \n          \n## 5- Feature Engineering \n          Derived Features\n          Categorical Feature encoding\n          \n## 6- Preparation, Models and Evaluation    \n          Preparation\n          Models and Evaluation\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"markdown","source":"## 1- Business Understanding  \nOur data contains an individual's annual income results based on various factors (Education level, Occupation,Gender, Age, etc.). \nGiven a new individual, our goal is to predict if that person makes more or less than 50K. ","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"markdown","source":"## 2- Data Acquisition  \nWe are going to acquire our dataset into **text** format, after downloading it from the **[UCI Machine Learning](https://archive.ics.uci.edu/ml/datasets/adult)** website. Here are the following libraries that we will be using to acquire the dataset and perform all the preprocessing and analysis.  ","metadata":{}},{"cell_type":"code","source":"import requests\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function will be used to acquire the data from the UCI website\ndef aquire_data(path_to_data, data_urls):\n    if not os.path.exists(path_to_data):\n        os.mkdir(path_to_data)\n        \n    for url in data_urls:\n        data = requests.get(url).content\n        filename = os.path.join(path_to_data, os.path.basename(url))\n        with open(filename, 'wb') as file: \n            file.write(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_urls = [\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n             \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\",\n             \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"]\n\naquire_data('data', data_urls)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the success of accessing the data\nprint('Output n° {}\\n'.format(1))\n! find data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can notice that all our data have been acquired from the UCI website. Here we have :  \n* **adult.names**: which corresponds to the different column names   \n* **adult.data**: corresponds to all the observations in the training data.  \n* **data.test**: corresponds to all the observation in the test data  \n","metadata":{}},{"cell_type":"code","source":"column_names = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \n                \"Martial Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \n                \"Capital-Gain\", \"Capital-Loss\", \"Hours-per-week\", \"Country\", \"Income\"] \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert Data into a Pandas Data Frame  ","metadata":{"trusted":true}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are going to acquire the training and the test datasets. \nThe corresponding column names have been specified in the previous **column_names** variable. Then, we use the regular expression **' \\*, \\*'** to trim all the whitespaces we can encounter in our datasets. As all the missing values have been specificied by **?**, so, **na_values** is used to take them into consideration during the data loading. Finally we specify **engine='python'** to avoid the warning that comes after using regular expression syntax.  ","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('data/adult.data', names=column_names, sep=' *, *', na_values='?', \n                   engine='python')\ntest = pd.read_csv('data/adult.test', names=column_names, sep=' *, *', skiprows=1, \n                   engine='python', na_values='?')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.Income.unique() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.Income.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to transform the **Income** column value for test data, in order to remove the **\".\"** at the end  ","metadata":{}},{"cell_type":"code","source":"test.Income = np.where(test.Income == '<=50K.', '<=50K', '>50K')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate train and test. We will split it before the training phase \ndf = pd.concat((train, test), axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Income.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Output n° {}\\n'.format(2))\n\n'''\nFirst 5 observations\n'''\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Output n° {}\\n'.format(3))\n\n'''\nLast 5 observations\n'''\ndf.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Output n° {}\\n'.format(4))\n\nprint('Our data contains {} observations and {} columns.'.format(df.shape[0],\n                                                                df.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3- Data Munging \nIn this step, we will perform two main tasks.  \n* **Dealing with missing values**    \nDuring data collection, it is very common to face missing data problem, that can occur for many reasons (confidentiality, error,etc.). So, it is very important to understand those problems, in order to fill them using appropriate techniques before applying any Machine Learning algorithm.    \n\n\n* **Dealing with outliers**     *\nOutliers are those values that are far away from the normal values that can be observed in the whole data. They can introduce high bias in our final model performance, and can even lead us to taking wrong conclusion during the analysis step.  \n\n#### A- Treating missing values   \nWe will use pandas **isnull()** function to look at all the missing values for each column.  ","metadata":{}},{"cell_type":"code","source":"print('Output n° {}\\n'.format(5))\nprint(df.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To the left, we have the name of the features and the number of missing values to the right. We can see that:   \n* **Workclass** has 1836 missing values   \n* **Occupation** has 1843 missing values  \n* **Country** has 583 missing values   \n\nTo deal with all the missing data, we couuld think of removing all the records (rows/observations) with those missing values. But, this technique could not be a better choice for our case, because we could lose much more data. To do so, we will use the following technique :  \n* Replace missing data of categorical columns data with the mode value (most occuring category) of that column.   \n* Replace missing numerical columns data with the median value of that column. Here we could use the mean instead of median, but the mean is very prompt to outliers (extreme values).     \n\nTo be able to identify which columns has which type, we can use pandas dtype() function.   \n\n","metadata":{}},{"cell_type":"code","source":"print('Output n° {}\\n'.format(6))\nprint(df.dtypes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To the left, we have the columns name, and their corresponding types to the right. So, we can see that the columns with missing values (discussed previously) are all categorical data (object).    \nThen, we can have a look at all the distincs (unique) values in each columns with pandas **unique()** function.  ","metadata":{}},{"cell_type":"code","source":"# Workclass  \nprint('Output n° {}\\n'.format(7))\nprint('Number of missing values: {}'.format(len(df['Workclass'].unique())))\nprint(df['Workclass'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Workclass has 9 unique values including **nan** (missing value)","metadata":{}},{"cell_type":"code","source":"# Occupation  \nprint(print('Output n° {}\\n'.format(8)))\nprint('Number of missing values: {}'.format(len(df['Occupation'].unique())))\nprint(df['Occupation'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Occupation column has 15 unique values, including **nan** ","metadata":{}},{"cell_type":"code","source":"# Country  \nprint('Output n° {}\\n'.format(9))\nprint('Number of missing values: {}'.format(len(df['Country'].unique())))\nprint(df['Country'].unique())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Country column has 42 unique values, including **nan** ","metadata":{}},{"cell_type":"markdown","source":"We know all the columns with missing values, and their type. We also have an idea of the unique values of each of those columns, now, we can perform the missing values replacement process.   \n\nTo do so, we will create a helper function that will perform this task for all the columns using python **statistics** built-in function.","metadata":{}},{"cell_type":"code","source":"import statistics as stat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_categorical_missing(data, column):\n    data.loc[data[column].isnull(), column] = stat.mode(data[column])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_to_fill = ['Workclass', 'Occupation', 'Country']\n\nfor col in cols_to_fill:\n    fill_categorical_missing(df, col)\n\nprint('Output n° {}\\n'.format(10))\n\n# Check the final data if there is any missing values \nprint(df.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that all the values to the right are equal to zero, which means that we have no missing values in our dataset.    ","metadata":{}},{"cell_type":"markdown","source":"### B- Dealing with outliers  \nTo be able to identify outliers in our dataset, we will use **seaborn** **boxplot** to all our numerical columns, and show the final result with **matplotlib**'s **show()** function.    \nWe the help of the **Output n°6 (i.e print(df.dtypes))**, we can see all our numrical columns; But a better way to look at them is to apply pandas **describe** function, which gives more statistical information about all the numerical columns.  \n\nIn this part, we are going to use the copy of our training dataset for outliers analysis, then create a helper function that will finally be applied to the original training data for outliers removal.","metadata":{}},{"cell_type":"code","source":"df_cp = df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cp.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 6 numerical columns (Age to Hours-per-week). To the left, we have many statistical information such as :  \n* **count**: for the total number of observation for each column.   \n* mean: the mean value of each column   \n* std: the standard deviation    \n* 25%, 50% and 75% are quantiles. \n\nWith the quantiles, min and max, the dataset can be splitted into 4 buckets:  \n* Bucket 1: below 25% (e.g) for **Age** column, 25% of people are under **28 years old**.\n* Bucket 2: between 25% and 50% (e.g), 25% of them (50%-25%) are between **28 and 37 years old**.  \n* Bucket 3: between 50% and 75% (e.g), 25% of them are between **37 and 48 years old** .  \n* Bucket 4: between above 75% (e.g), 25% of them are over **48 years old**.  \n\n**Then all the values beyond 1.5xIQR are considered as outliers. ** \nIQR = Inter Quartile Range = 75th - 25th.   \n\nThis images gives a better understanding of a boxplot.   \n![](https://www.researchgate.net/publication/318986284/figure/fig1/AS:525404105646080@1502277508250/Boxplot-with-outliers-The-upper-and-lower-fences-represent-values-more-and-less-than.png)\n\nThen we will create a helper function that will remove all the outliers from our dataset. But, before that, let have a look at the boxplot.   ","metadata":{}},{"cell_type":"code","source":"import seaborn as sns \nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Age \nsns.boxplot(y='Age', data=df_cp)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let calculate 0-100th percentile to find a correct percentile value for removal of outliers","metadata":{}},{"cell_type":"code","source":"def ten_to_ten_percentiles(data, column):\n    for i in range(0,100,10):\n        var = data[column].values\n        var = np.sort(var, axis=None)\n        print('{} percentile value is {}'.format(i, var[int(len(var) * (float(i)/100))]))\n    print('100 percentile value is {}'.format(var[-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ten_to_ten_percentiles(df_cp, 'Age')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could see from the boxplot of Age that there is no extreme value. Then after checking with percentile values, we have a confirmation of our remark. ","metadata":{}},{"cell_type":"code","source":"#calculating column values at each percntile 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100\ndef percentiles_from_90(data, column):\n    for i in range(90,100):\n        var = data[column].values\n        var = np.sort(var, axis=None)\n        print('{} percentile value is {}'.format(i, var[int(len(var) * (float(i)/100))]))\n    print('100 percentile value is {}'.format(var[-1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Going deeper with the percentile values, we can have more information. So, here is a function that will give us the percentile values for each values from 99 to 100 percentile. ","metadata":{}},{"cell_type":"code","source":"#calculating colunm values at each percntile 99.0,99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100\ndef percentiles_from_99(data, column):\n    for i in np.arange(0.0, 1.0, 0.1):\n        var =data[column].values\n        var = np.sort(var,axis = None)\n        print(\"{} percentile value is {}\".format(99+i,var[int(len(var)*(float(99+i)/100))]))\n    print(\"100 percentile value is \",var[-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Education-Num\nsns.boxplot(y='Education-Num', data=df_cp)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ten_to_ten_percentiles(df_cp, 'Education-Num')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no anomalies with Education number. ","metadata":{}},{"cell_type":"code","source":"# Capital-Gain\nsns.boxplot(y='Capital-Gain', data=df_cp)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ten_to_ten_percentiles(df_cp, 'Capital-Gain')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percentiles_from_90(df_cp, 'Capital-Gain')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percentiles_from_99(df_cp, 'Capital-Gain')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing the outliers based on 99.5th percentile of Capital-Gain\ndf_cp = df_cp[df_cp['Capital-Gain']<=34095]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Capital-Gain\nsns.boxplot(y='Capital-Gain', data=df_cp)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Capital-Loss\nsns.boxplot(y='Capital-Loss', data=df_cp)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ten_to_ten_percentiles(df_cp, 'Capital-Loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percentiles_from_90(df_cp, 'Capital-Loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"percentiles_from_99(df_cp, 'Capital-Loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No special extreme value here as we could notice for Capital-Gain. ","metadata":{}},{"cell_type":"code","source":"# Hours-per-week\nsns.boxplot(y='Hours-per-week', data=df_cp)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ten_to_ten_percentiles(df_cp, 'Hours-per-week')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no special extreme value here. ","metadata":{}},{"cell_type":"markdown","source":"Now, we are going to create a helper function in order to remove all the outliers, based in our previous univariate analysis.  ","metadata":{}},{"cell_type":"code","source":"def remove_outliers(data):\n    a = data.shape[0]\n    print(\"Number of salary records = {}\".format(a))\n        \n    temp_data = data[data['Capital-Gain']<=34095]\n    b = temp_data.shape[0]\n    \n    print('Number of outliers from the Capital-Gain column= {}'.format(a - b))\n        \n    data = data[(data['Capital-Gain']<=34095)]\n    \n    print('Total outlies removed = {}'.format(a-b))\n    print('-----'*10)\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Removing all the outliers from the data')\nprint('-----'*10)\ndf_no_outliers = remove_outliers(df)\n\nproportion_remaing_data = float(len(df_no_outliers)) / len(df)\nprint('Proportion of observation that remain after removing outliers = {}'.format(proportion_remaing_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After removing the outliers from out data, still 99.49% of the dataset remain present. ","metadata":{}},{"cell_type":"markdown","source":"## 4- Exploratory Data Analysis   ","metadata":{}},{"cell_type":"markdown","source":"First thing first! \nLet's take a look at the number of people who make more that 50K and those who don't","metadata":{}},{"cell_type":"code","source":"df_no_outliers.Income.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"palette = {\"<=50K\":\"r\", \">50K\":\"g\"}\nsns.countplot(x=\"Income\", data=df_no_outliers, hue=\"Income\", palette=palette)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can notice that we have 24720 adults who make less than 50K dollars and only 7841 of them make more than 50K dollars. So,only 24% of adult make more than 50K dollars.","metadata":{}},{"cell_type":"markdown","source":"#### A- Numerical Data   \nFor this part, we will be performing centrality measure (mean, median) and dispersion measures (range, percentiles, variance, standard deviation).  \nAll those information can be found with pandas **describe()** function.  ","metadata":{}},{"cell_type":"code","source":"df_no_outliers.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this result, we can see that our features are in different scales, so that information will be useful for feature engineering step. For simple visualization purpose, we can plot the probability density of all those features. ","metadata":{}},{"cell_type":"markdown","source":"##### A.1- Univariate Analysis ","metadata":{}},{"cell_type":"code","source":"# Age  \ndf_no_outliers.Age.plot(kind='kde', title='Density plot for Age', color='c')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we have a positive skewed distribution for Age feature. ","metadata":{}},{"cell_type":"code","source":"# Capital-Gain  \ndf_no_outliers['Capital-Gain'].plot(kind='kde', title='Density plot for Capital-Gain', color='c')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Capital-Loss  \ndf_no_outliers['Capital-Loss'].plot(kind='kde', title='Density plot for Capital-Loss', color='c')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Capital-Loss  \ndf_no_outliers['Hours-per-week'].plot(kind='kde', title='Density plot for Hours-per-week', color='c')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to deal with the problem of distribution for all our numerical data values in the feature engineering part. ","metadata":{}},{"cell_type":"markdown","source":"##### A.2- Bivariate analysis  \nWe will try to determine the correlation between some numerical data.","metadata":{}},{"cell_type":"code","source":"# Capital-Gain and Education-Num \n# use scatter plot for bi-variate distribution\ndf_no_outliers.plot.scatter(x='Education-Num', y='Capital-Gain', color='c', title='scatter plot : Education-Num vs Capital-Gain');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a positive relationship between the number of year of education and the Capital Gain. The more educated you are, your are likely to have more capital. ","metadata":{}},{"cell_type":"code","source":"# Hours-per-week and Education-Num \n# use scatter plot for bi-variate distribution\ndf_no_outliers.plot.scatter(x='Education-Num', y='Hours-per-week', color='c', title='scatter plot : Education-Num vs Hours-per-week');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no interesting pattern. ","metadata":{}},{"cell_type":"code","source":"# Capital-Gain and Hours-per-week\n# use scatter plot for bi-variate distribution\ndf_no_outliers.plot.scatter(x='Hours-per-week', y='Capital-Gain', color='c', title='scatter plot : Hours-per-week vs Capital-Gain');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can not identify any interesting pattern from this visualization. ","metadata":{}},{"cell_type":"code","source":"# Capital-Gain and Capital-Loss\n# use scatter plot for bi-variate distribution\ndf_no_outliers.plot.scatter(x='Capital-Gain', y='Capital-Loss', color='c', title='scatter plot : Capital-Loss vs Capital-Gain');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"People without any capital Gain lose a lot of money, which is obvious, because without any capital Gain, you would need to borrow with interest, and then keep **\"surviving\".** ","metadata":{}},{"cell_type":"code","source":"numerical_cols = ['int64']  \nplt.figure(figsize=(10, 10))\nsns.heatmap( \n            df_no_outliers.select_dtypes(include=numerical_cols).corr(),\n            cmap=plt.cm.RdBu, \n            vmax=1.0,\n            linewidths=0.1,\n            linecolor='white', \n            square=True,\n            annot=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the correlation matrix, we can see that the level of relationship is very low between the numerical features.  ","metadata":{}},{"cell_type":"markdown","source":"\n#### B- Categorical Data","metadata":{}},{"cell_type":"markdown","source":"There are many explorations we can do in order to have a better understanding of the data.   \nHere are some possibilities we could have:  \n* B.1- Income VS Occupation for countries in each continent\n* B.2- Income VS Workclass for countries in each continent\n* B.3- Income VS Marital Status for countries in each continent\n* B.4- Mean Capital Gain VS Martial Status for each continent\n","metadata":{}},{"cell_type":"code","source":"df_no_outliers.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have many countries from different continent. For better visualization, it might be interesting to create a new column **Continent** in order to easily group information per continent and the corresponding countries. ","metadata":{}},{"cell_type":"code","source":"df_no_outliers['Country'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is country name called **South** which is definitly an error. It could be considered as **continent**, then we could associate in with the corresponding continent. But, here is the problem: we have both **South-America**, **South-Asia** that could be possible values. In order to avoid including more errors in our data, it might be better to remove the corresponding observations in case that action does not lead to loosing too much data.  ","metadata":{}},{"cell_type":"code","source":"south_df = df_no_outliers[df_no_outliers['Country']=='South']\na = south_df.shape[0]\nb = df_no_outliers.shape[0]\n\nprint('{} rows corresponds to South, which represents {}% of the data'.format(a, (1.0*a/b)*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can remove all the corresponding rows for **Country == South** because, it corresponds to only 0.244% of the original dataset. ","metadata":{}},{"cell_type":"code","source":"south_index = south_df.index \ndf_no_outliers.drop(south_index, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are going to perform the following preprocessing:  \n* Outlying-US(Guam-USVI-etc) ==> Outlying-US   \n* Trinadad&Tobago ==> Trinadad-Tobago  \n* Hong ==> Hong-Kong","metadata":{}},{"cell_type":"code","source":"# Changing the corresponding values.\ndf_no_outliers.loc[df_no_outliers['Country']=='Outlying-US(Guam-USVI-etc)', 'Country'] = 'Outlying-US'\ndf_no_outliers.loc[df_no_outliers['Country']=='Trinadad&Tobago', 'Country'] = 'Trinadad-Tobago'\ndf_no_outliers.loc[df_no_outliers['Country']=='Hong', 'Country'] = 'Hong-Kong'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if the process worked\ndf_no_outliers['Country'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see that the changes have been made. ","metadata":{}},{"cell_type":"code","source":"asia = ['India', 'Iran', 'Philippines', 'Cambodia', 'Thailand', 'Laos', 'Taiwan', \n       'China', 'Japan', 'Vietnam', 'Hong-Kong']  \n\namerica = ['United-States', 'Cuba', 'Jamaica', 'Mexico', 'Puerto-Rico', 'Honduras', \n           'Canada', 'Columbia', 'Ecuador', 'Haiti', 'Dominican-Republic', \n           'El-Salvador', 'Guatemala', 'Peru', 'Outlying-US', 'Trinadad-Tobago', \n           'Nicaragua', '']  \n\neurope = ['England', 'Germany', 'Italy', 'Poland', 'Portugal', 'France', 'Yugoslavia', \n          'Scotland', 'Greece', 'Ireland', 'Hungary', 'Holand-Netherlands'] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now, create a dictionary to map each country to a Corresponding continent. \ncontinents = {country: 'Asia' for country in asia}\ncontinents.update({country: 'America' for country in america})\ncontinents.update({country: 'Europe' for country in europe})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Then use Pandas map function to map continents to countries  \ndf_no_outliers['Continent'] = df_no_outliers['Country'].map(continents)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we have the continents corresponding to all the existing contries in our dataset.","metadata":{}},{"cell_type":"code","source":"df_no_outliers['Continent'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B.1- Income VS Occupation for countries in each continent  \nI created a helper fonction in order to preprocess for each country in one shot. ","metadata":{}},{"cell_type":"code","source":"def Occupation_VS_Income(continent):\n    choice = df_no_outliers[df_no_outliers['Continent']==continent] \n    countries = list(choice['Country'].unique())\n\n    for country in countries:\n        pd.crosstab(choice[choice['Country']==country].Occupation, choice[choice['Country']==country].Income).plot(kind='bar', \n                                                                                                                       title='Income VS Occupation in {}'.format(country))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B.1.1- For Asia","metadata":{}},{"cell_type":"code","source":"Occupation_VS_Income('Asia')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B.1.2- For America","metadata":{}},{"cell_type":"code","source":"Occupation_VS_Income('America')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B.1.3- For Europe","metadata":{}},{"cell_type":"code","source":"Occupation_VS_Income('Europe')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B.2- Income VS Workclass for countries in each continent  ","metadata":{}},{"cell_type":"code","source":"def Workclass_VS_Income(continent):\n    choice = df_no_outliers[df_no_outliers['Continent']==continent] \n    countries = list(choice['Country'].unique())\n\n    for country in countries:\n        pd.crosstab(choice[choice['Country']==country].Workclass, choice[choice['Country']==country].Income).plot(kind='bar', \n                                                                                                                       title='Income VS Workclass in {}'.format(country))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B.2.1- For Asia","metadata":{}},{"cell_type":"code","source":"Workclass_VS_Income('Asia')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B.2.2- For America","metadata":{}},{"cell_type":"code","source":"Workclass_VS_Income('America')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B.2.3- For Europe","metadata":{}},{"cell_type":"code","source":"Workclass_VS_Income('Europe')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B.3- Income VS Marital Status for countries in each continent  ","metadata":{}},{"cell_type":"code","source":"def MaritalStatus_VS_Income(continent):\n    choice = df_no_outliers[df_no_outliers['Continent']==continent] \n    countries = list(choice['Country'].unique())\n\n    for country in countries:\n        pd.crosstab(choice[choice['Country']==country]['Martial Status'], choice[choice['Country']==country].Income).plot(kind='bar', \n                                                                                                                       title='Income VS Workclass in {}'.format(country))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B.3.1- For Asia","metadata":{}},{"cell_type":"code","source":"MaritalStatus_VS_Income('Asia')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B.4- Mean Capital Gain VS Martial Status for each continent","metadata":{}},{"cell_type":"markdown","source":"To accomplish this task; I will create a new dataframe containing the grouping result of Continent, Contient, Marital Status and the **mean value of Capital Gain**","metadata":{}},{"cell_type":"code","source":"# reset_index(): to convert to aggregation result to a pandas dataframe.\nagg_df = df_no_outliers.groupby(['Continent','Country', 'Martial Status'])['Capital-Gain'].mean().reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agg_df['Mean_Capital_Gain'] = agg_df['Capital-Gain']\nagg_df.drop('Capital-Gain', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agg_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Mean_TotCapital_VS_Marital_Status(continent):\n    choice = agg_df[agg_df['Continent']==continent] \n    countries = list(choice['Country'].unique())\n\n    for country in countries:\n        df_c = choice[choice['Country']==country]\n        ax = sns.catplot(x='Martial Status', y='Mean_Capital_Gain', \n                         kind='bar', data=df_c)\n\n        ax.fig.suptitle('Country: {}'.format(country))\n        ax.fig.autofmt_xdate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B.4.1- For Asia","metadata":{}},{"cell_type":"code","source":"Mean_TotCapital_VS_Marital_Status('Asia')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B.4.2- For America","metadata":{}},{"cell_type":"code","source":"Mean_TotCapital_VS_Marital_Status('America')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### B.4.3- For Europe","metadata":{}},{"cell_type":"code","source":"Mean_TotCapital_VS_Marital_Status('Europe')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5- Feature Engineering   \nThis is one of the most crucial aspect for a Data Science project. It is a process of transforming the raw data to better representative \nfeatures in order to create better predictive models. \n\n#### A- Derived Features   \nSometimes, it is important to perform some transformations on the features/columns in order to reduce the number of original data columns. \nLet's start looking at our columns.","metadata":{"trusted":true}},{"cell_type":"markdown","source":"##### A.1- Education and Education-Num  ","metadata":{}},{"cell_type":"code","source":"edu = df_no_outliers.Education.unique()\neduNum = df_no_outliers['Education-Num'].unique()\nprint('Education: \\nTotal category:{}\\nValues: {}\\n'.format(len(edu),list(edu)))\nprint('Education Num: \\nTotal Education-Num:{}\\nValues: {}'.format(len(eduNum),\n                                                                  list(eduNum)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that The **Education-Num** seems to be the numerical representation of **Education**, and also the same Total number (16). To do so, we will need only one of them, not both columns.  \nLet's check some observations (rows) to verify our hypothesis if there is a corrrespondance between **Education-Num** and **Education**.   \nThen we can simply visualize the two columns in order to check the correspondance between them.  ","metadata":{}},{"cell_type":"code","source":"ax = sns.catplot(x='Education', y='Education-Num', kind='bar', data=df_no_outliers)\nax.fig.suptitle('Numerical Representation of Educations')\nax.fig.autofmt_xdate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the previous plot, we can see that \n* Bachelor <==> 13  \n* HS-grad <==> 9\n* 7th-8th <==> 4   \n* 9th <==> 5    \n* Preschool <==> 1 \n* etc.  \nBased on those information, we will need only one column to represent the **level of education**, and in our case,   \nwe will choose **Education-Num** (remove **Education** column) which corresponds to the numerical representation.  ","metadata":{}},{"cell_type":"code","source":"# Finally remove the Education column  \ndf_no_outliers.drop('Education', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### A.2- Capital-Loss and Capital-Gain  \nFrom those two features, we can create a new column called **Capital-State** that will be the difference between Capital-Gain and Capital-Loss.  \nThen we will remove those two features.  ","metadata":{}},{"cell_type":"code","source":"df_no_outliers['Capital-State'] = df_no_outliers['Capital-Gain'] - df_no_outliers['Capital-Loss']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Then remove Capital-Gain and Capital-Loss. \ndf_no_outliers.drop(['Capital-Gain', 'Capital-Loss'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nLet not forget to drop the 'Continent' column we added for \nvisualization purpose. \n'''\ndf_no_outliers.drop('Continent', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_outliers.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### A.3- Age State (Adult or Child)   \nA person older than 18 is an adult. Otherwise he/she is a child.  ","metadata":{}},{"cell_type":"code","source":"# AgeState based on Age\ndf_no_outliers['AgeState'] = np.where(df_no_outliers['Age'] >= 18, 'Adult', 'Child')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AgeState Counts  \ndf_no_outliers['AgeState'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='AgeState', data=df_no_outliers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**fnlwgt** column is not an important feature. ","metadata":{}},{"cell_type":"code","source":"df_no_outliers.drop('fnlwgt', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_outliers.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Information about our data\ndf_no_outliers.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### B- Categorical Feature encoding    \nA machine learning model only works with numerical features. To do so, we need to encode all our categorical features. Those features are represented by **object**  with the help of the previous **info** command.    \nWe are going to perform the **One Hot Ending** method on all the categorical features by using Pandas **get_dummies()** function.  \nWe are not going to take in consideration **Income** column, because it is the column we try to predict.  ","metadata":{}},{"cell_type":"code","source":"# Columns: Workclass, Martial Status Occupation, Relationship, Race, Sex, Country, AgeState\ndf_no_outliers = pd.get_dummies(df_no_outliers, columns=['Workclass', 'Martial Status', 'Occupation', \n                                 'Relationship', 'Race', 'Sex', 'Country', 'AgeState'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_no_outliers['Income'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n1: For those who make more than 50K \n0: For those who don't\n'''\ndf_no_outliers['Income'] = np.where(df_no_outliers['Income'] =='>50K', 1, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reorder columns : In order to have 'Income' as last feature.\ncolumns = [column for column in df_no_outliers.columns if column != 'Income']\ncolumns = columns + ['Income'] \ndf = df_no_outliers[columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Information about our data\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6- Preparation, Models and Evaluation    \n#### 6.1- Data Preparation   \nWe need to split our dataset for training and testing data.  \n80% of the data will be used for training and 20% for testing.","metadata":{"trusted":true}},{"cell_type":"code","source":"y = df.Income.ravel()\nX = df.drop('Income', axis=1).as_matrix().astype('float')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('X shape: {} | y shape: {}'.format(X.shape, y.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('X train shape: {} | y shape: {}'.format(X_train.shape, y_train.shape))\nprint('X test shape: {} | y shape: {}'.format(X_test.shape, y_test.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.2- Models & Evaluation   \nBefore building any machine learning model. It is important to build a baseline model first, in order judge the performance of the upcoming models.  ","metadata":{}},{"cell_type":"markdown","source":"##### Baseline Model","metadata":{}},{"cell_type":"code","source":"from sklearn.dummy import DummyClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummy_clf = DummyClassifier(strategy='most_frequent', random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model \ndummy_clf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Score of baseline model : {0:.2f}'.format(dummy_clf.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Logistic Regression ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_clf = LogisticRegression(random_state=0)\nparameters = {'C':[1.0, 10.0, 50.0, 100.0, 1000.0], 'penalty' : ['l1','l2']}\nlr_clf = GridSearchCV(lr_clf, param_grid=parameters, cv=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_clf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_clf.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best score : {0:.2f}'.format(lr_clf.best_score_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Score for logistic regression - on test : {0:.2f}'.format(lr_clf.score(X_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2024-05-29T16:59:12.005899Z","iopub.execute_input":"2024-05-29T16:59:12.006244Z","iopub.status.idle":"2024-05-29T16:59:12.124378Z","shell.execute_reply.started":"2024-05-29T16:59:12.006193Z","shell.execute_reply":"2024-05-29T16:59:12.122885Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3966e8be4127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Score for logistic regression - on test : {0:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'lr_clf' is not defined"],"ename":"NameError","evalue":"name 'lr_clf' is not defined","output_type":"error"}]}]}